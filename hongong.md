# 회귀
- 두 변수 사이의 상관관계를 분석하는 방법
- 임의의 수치를 예측하는 문제(타깃값도 임의의 수치가 된다)

## .1 k-최근접 이웃 회귀
> 예측값에 가까운 샘플 k개의 평균을 타깃값으로 정하는 회귀

#### 결정계수
> 대표적인 회귀 문제의 성능 측정 도구 (1에 가까울수록 좋고, 0에 가까울수록 성능이 나쁘다)
#### 과대적합 / 과소적합
> 과대적합(Overfitting)
>> 훈련 세트 점수는 좋았으나 테스트 세트가 나쁜 경우
>
> 과소적합
>> '훈련 세트 점수 < 테스트 세트'거나 둘 다 점수가 나쁜 경우

## .2 선형회귀
> 특성과 타깃 사이의 관계를 잘 나타내는 선형 방적식을 찾는 회귀(특성이 하나면 직선 방정식이 된다)
- 선형 회귀가 찾은 특성과 타깃 사이의 관계는 <a style=color:purple>계수나 가중치</a>로 저장된다
#### 모델 파라미터
> 선형 회귀의 가중치처럼 <a style=color:purple>모델이 학습한 파라미터 </a>
#### 다항 회귀(!= 다중 회귀)
> 다항식을 사용한 선형회귀
- 다항식을 사용해서 n차 방정식이 되더라도 다른 변수로 치환할 수 있기 때문에 그대로 선형 회귀다
    - y= wx<sup>n</sup>+b에서 x가 몇 제곱이더라도 a=x<sup>n</sup>으로 치환하면 y = wa+b로 선형 회귀다

## .3 특성 공학
> 주어진 특성을 조합해 새로운 특성을 만드는 전체 과정
#### 다중 회귀
> 다수의 특성을 사용하는 회귀모델(선형 모델은 특성이 많을수록 성능이 강해진다)
#### 규제
> - 릿지
>   > 선형 모델의 계수를 작게 만들어 과댜적합을 완화하는 규제가 있는 선형 회귀 모델의 한 종류
> - 라쏘
>   > 릿지처럼 규제가 있는 선형 회귀 모델이지만 계수값을 아예 0으로 만들 수도 있다
#### 하이퍼파라미터
> 알고리즘이 학습하지 않은 파라미터(사람이 지정해야 한다)

# 분류 알고리즘
## .1 로지스틱 회귀(= 이진 분류)
> 선형 방정식을 사용한 분류 알고리즘

 - sigmoid나 softmax를 사용해 클래스(타깃 변수의 값) 확률을 출력할 수 있다
> sigmoid
>> 선형 방정식의 출력을 0~1 사이로 압축해서 이진 분류를 위해 사용
>
> softmax
>> 여러 선형 방정식의 출력 결과를 정규화하여 합이 1이 되도록 한다. (주로 마지막 층에 입력)
#### 다중 분류
> 타깃 클래스가 2개 이상인 분류 문제( 클래스마다 z값(예측값)을 하나씩 계산)

## .2 확률적 경사 하강법(= SGD)
> 점진적 학습의 한 종류로, 훈련 세트에서 샘플을 하나씩 꺼내 손실 함수의 기울기를 따라 최적의 모델을 찾는 알고리즘
- 샘플을 여러 개 사용하면 미니배치 경사 하강법, 한 번에 전체를 사용하면 배치 경사 하강법
- 신경망 알고리즘에서는 반드시 SGD를 사용한다
- 점진적 학습 : 이전 훈련 모델을 버리고 새로운 모델을 훈련하는 방식
- 에포크(epoch)
  > 훈련 세트를 한 번 모두 사용하는 과정
#### 손실 함수(= 비용 함수)
> SGD가 최적화할 대상
- 손실함수의 기울기는 반드시 실수(미분 가능해야한다)
#### 로지스틱 손실 함수(CEE)