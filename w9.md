# 1. DcGan
> Deep Convolutional Generative Adversarial Networks

# 2. ML / DL
> DL = ML + nn(신경망)
> ML = DL + 확률
> ML이 DL보다 넓은 개념
- 묵시적
- 샘플데이터를 기반으로 모델 구축
  - (샘플 데이터 = 훈련 데이터)
- ML과 DL 차이점은 통계다 
  - ML은 통계학습

# 3. 데이터 유형
> 이산형(Categorical)
>> 명목(norminal), 순위(ordinal) 
> 
> 연속형(Continuous)
>> 간격(Interval), 비율(Ratio)

## 연역과 귀납
연역은 가정된 전제이다.
귀납은 개인적 경험이다.

## 데이터 분석의 두 가지 접근 방법
1) 확증적 데이터 분석(CDA: Confirmatory Data Analysis) = 추론통계 = (가설 → 특정사례) = 연역
2) 탐색적 데이터 분석(EDA: Exploratory Data Analysis) = 기술통계 = (데이터(특정사례) → 모델) = 귀납 
> 인공지능은
>> 지도 : 귀납 → 연역 / 비지도 : 연역 → 연역
> 
>> 지도와 비지도의 차이는 연역과 귀납 중 무엇으로 시작하는지다

세상에 없는걸 만들면 ML, 이미 있는걸 다시 확인하면 통계

### 확률(prob)
- 종류
  - 수학적(선험적) 확률(시작점이 가설) → 사전 : 식으로 존재 → 연역
  - 통계적(경험적) 확률(시작점이 데이터) → 사후 : 식*큰수(data, limit) → 귀납
- 큰수의 법칙 
> 수학적 확률과 통계적 확률이 같아지는 지점이 있다는게 대수법칙
> P(A) = lim(k/n)에서 P(A)는 통계적, 계수lim은 통계적 확률, k/n은 수학
> 로그는 큰 수다 (log2 1024=10에서 10은 log2를 생략하고 1024를 나타내서)
- target(기대값) = 계수 * variable(변수) + constant(상수)
### 통계
> 데이터 + 확률 
### ML을 위한 통계 개념
표본<br/>
우도함수

    > 우도 함수(가능도 함수로 번역되기도 하고, 영어로는 likelihood function 이라 합니다)는 실현된 데이터(혹은 관찰된 데이터 observed data)로 부터 특정 통계 모델의 적합성을 확인하는데 주로 이용됩니다.

대수(큰 수)의 법칙<br/>
베이지안<br/>
분포<br/>
랜덤

---
# 4. 학습
학습은 통계학에서 추정 문제를 해결하는 과정(=추론)이다
- 지도 학습은 샘플을 사용, 비지도는 샘플을 사용 x
- 지도학습은 분류(classification)와 회귀(regress)로 나뉜다
  > 결국 learning =  target을 구하는 modeling이다
  >
  > model은 var를 잡아내서 class(객체)를 시도한다
- 변수는 feature 와 target 으로 나뉜다.
- 상수는 계수와 편향로 나뉜다.
  - 따라서 다음과 같은 식의 구조를 같는다.
  target(예측값, Hat) = 계수 * feature-value + 편향
  특성변수 = 독립변수 = 외생변수 = x변수
  목적변수 = 종속변수 = 내생변수 = y변수
  - 기대값 y는 다시 y와 y Hat(가설)으로 나눠진다
  feature 의 변환은 표준화와 정규화가 있다.
  아웃라이어가 있으면 표준화 나머지는 정규화가 낫다.
  - 계수(係數, coefficient)는 '인자(因子)'라는 뜻으로 쓰인다.
  - 보통 식 앞에 곱해지는 상수를 말한다.
  - 가장 흔한 계수의 개념은 다항식에서 x n 앞에 붙는 수이다.
  학습은 통계학에서  추정문제 해결과정(=추론)이다.
  learning 은 target 을 구하는 modeling 이다.

### Ground Truth
### 정규화(Normalization)와 표준화(Standardization = Z-score)
> feature의 변환은 정규화와 표준화가 있다
> 아웃라이어가 있으면 표준화, 나머지는 정규화가 낫다


## 분포
> 분포는 함수다
>> 확률 질량 함수(PMF) : 리턴값이 정수
>
>> 확률 밀도 함수(PDF) : 리턴값이 실수
- dense는 신경망에서 사용한다
> 확률분포는
>> 이산 - 확률질량함수 PMF: 이항분포, 다항분포, 이산균등분포, 푸아송분포, 베르누이분포, (초)기하분포
>
>> 연속 - 확률밀도함수 PDF: 정규분포(=가우스분포), 연속균등분포, 카이제곱분포, 감마분포


## 편향과 편차, 분산
<a href="https://opentutorials.org/module/3653/22071">링크</a><br/>
정답 하나를 맞추기 위해 컴퓨터는 여러 번의 예측값 내놓기를 시도하는데,
컴퓨터가 내놓은 예측값의 동태를 묘사하는 표현이 '편향' 과 '분산'이다

예측값들과 정답이 대체로 떨어져 있으면 결과의 편향(bias)이 높다고 말하고,
예측값들이 자기들끼리 대체로 흩어져있으면 결과의 분산(variance)이 높다고 말합니다.

회귀 문제이든, 분류 문제이든
첫 번째 그림과 같은 상황을 Underfitting = High Bias<br/>
세 번째 그림과 같은 상황(y와 y Hat이 완전 일치)을 Overfitting = High Variance라고 한다
> 오버피팅의 경우는 맞춘게 아니라 답을 알고있었다고 해서 오히려 안좋은 상황이다 95%가 가장 이상적이다

#### 추정에 있어 통계학의 손실함수에는 평균제곱오차 또는 음의 로그 우도함수가 있으며 머신러닝에서도 동일한 손실함수를 사용한다

### MSE vs. CCEE
회귀ML의 손실함수는 MSE
분류ML의 손실함수는 CCEE(Categorical Cross Entropy Error), 활성화 함수로 softmax를 사용한다

### 데이터셋
지금까지 데이터는 train set과 test set 두개로만 나누었지만 train, test 두개로만 분리하는 것은 기초적인 수준, 
보통 현업에서 모델(커스텀모델)을 만들 때는 아래 세 개로 나눈다
- train → 기출
- test(=Unseen data, 학습과 검증이 끝난 모델의 최종 성능 평가용) → 수능
- validation set(학습이 완료된 모델을 검증하기 위한 set(중간점검용)) → 모의고사
  - 수시로 사용해야 되므로 Cross validation을 사용한다

### 불순도(gini)



---

- ~법은 메소드, ~기능은 함수로 해석해라
- 절차가 있는 메소드는 hook, ML에서의 모델은 클래스(이후에는 파일로도)
## 객체 모델(Object Model)

# criterion
> = 표준

## CCEE(Categorical Cross Entropy Error)

### Entropy
> 사전적인 정의 : 함수 / 통계적 의미 : 기대값

### 지니계수

## Decision tree
random forest 알고리즘의 기본이 되는 알고리즘으로 tree 기반 알고리즘

## 하이퍼파라미터
model : D에서 
d = D(파라미터)  
d.hook() 일 때 hook이 생략된 D의 파라미터를 받는 것